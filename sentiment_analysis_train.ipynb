{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the positive data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 2, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_EPOCHS=2\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ubuntu/anaconda3/playground_tf/runs/1480850493\n",
      "\n",
      "2016-12-04T11:21:35.437602: step 1, loss 1.8793, acc 0.5\n",
      "2016-12-04T11:21:35.877191: step 2, loss 1.83019, acc 0.5\n",
      "2016-12-04T11:21:36.313431: step 3, loss 2.34375, acc 0.4375\n",
      "2016-12-04T11:21:36.750717: step 4, loss 2.21472, acc 0.515625\n",
      "2016-12-04T11:21:37.186575: step 5, loss 1.81136, acc 0.546875\n",
      "2016-12-04T11:21:37.627223: step 6, loss 1.99464, acc 0.53125\n",
      "2016-12-04T11:21:38.063357: step 7, loss 2.45226, acc 0.46875\n",
      "2016-12-04T11:21:38.498864: step 8, loss 2.1185, acc 0.453125\n",
      "2016-12-04T11:21:38.935873: step 9, loss 2.07719, acc 0.421875\n",
      "2016-12-04T11:21:39.371375: step 10, loss 2.07665, acc 0.453125\n",
      "2016-12-04T11:21:39.807194: step 11, loss 1.34157, acc 0.578125\n",
      "2016-12-04T11:21:40.243398: step 12, loss 2.19954, acc 0.4375\n",
      "2016-12-04T11:21:40.677934: step 13, loss 1.73994, acc 0.53125\n",
      "2016-12-04T11:21:41.114282: step 14, loss 2.41245, acc 0.5\n",
      "2016-12-04T11:21:41.548439: step 15, loss 1.31176, acc 0.640625\n",
      "2016-12-04T11:21:41.985664: step 16, loss 1.65448, acc 0.5625\n",
      "2016-12-04T11:21:42.421965: step 17, loss 1.53487, acc 0.4375\n",
      "2016-12-04T11:21:42.858277: step 18, loss 2.11218, acc 0.4375\n",
      "2016-12-04T11:21:43.295003: step 19, loss 1.43348, acc 0.609375\n",
      "2016-12-04T11:21:43.728735: step 20, loss 1.56163, acc 0.59375\n",
      "2016-12-04T11:21:44.166334: step 21, loss 2.43064, acc 0.4375\n",
      "2016-12-04T11:21:44.602106: step 22, loss 2.105, acc 0.4375\n",
      "2016-12-04T11:21:45.036908: step 23, loss 1.72615, acc 0.515625\n",
      "2016-12-04T11:21:45.472849: step 24, loss 1.32947, acc 0.53125\n",
      "2016-12-04T11:21:45.905608: step 25, loss 1.82376, acc 0.5625\n",
      "2016-12-04T11:21:46.341928: step 26, loss 1.41523, acc 0.5625\n",
      "2016-12-04T11:21:46.780413: step 27, loss 1.82494, acc 0.546875\n",
      "2016-12-04T11:21:47.216479: step 28, loss 1.98303, acc 0.421875\n",
      "2016-12-04T11:21:47.652833: step 29, loss 1.77635, acc 0.609375\n",
      "2016-12-04T11:21:48.089571: step 30, loss 1.86214, acc 0.46875\n",
      "2016-12-04T11:21:48.526064: step 31, loss 1.5186, acc 0.5625\n",
      "2016-12-04T11:21:48.963014: step 32, loss 1.43923, acc 0.546875\n",
      "2016-12-04T11:21:49.398452: step 33, loss 2.37421, acc 0.421875\n",
      "2016-12-04T11:21:49.829972: step 34, loss 1.31195, acc 0.625\n",
      "2016-12-04T11:21:50.266211: step 35, loss 1.43611, acc 0.546875\n",
      "2016-12-04T11:21:50.699825: step 36, loss 2.38637, acc 0.34375\n",
      "2016-12-04T11:21:51.132704: step 37, loss 1.56261, acc 0.609375\n",
      "2016-12-04T11:21:51.568487: step 38, loss 1.68547, acc 0.5625\n",
      "2016-12-04T11:21:52.003381: step 39, loss 1.69985, acc 0.5625\n",
      "2016-12-04T11:21:52.433779: step 40, loss 2.06715, acc 0.515625\n",
      "2016-12-04T11:21:52.865438: step 41, loss 1.57207, acc 0.46875\n",
      "2016-12-04T11:21:53.299149: step 42, loss 1.6035, acc 0.578125\n",
      "2016-12-04T11:21:53.733813: step 43, loss 2.0304, acc 0.5\n",
      "2016-12-04T11:21:54.166893: step 44, loss 1.7107, acc 0.484375\n",
      "2016-12-04T11:21:54.603012: step 45, loss 1.87031, acc 0.515625\n",
      "2016-12-04T11:21:55.038451: step 46, loss 1.80717, acc 0.453125\n",
      "2016-12-04T11:21:55.472494: step 47, loss 1.52392, acc 0.578125\n",
      "2016-12-04T11:21:55.905274: step 48, loss 1.60854, acc 0.53125\n",
      "2016-12-04T11:21:56.339435: step 49, loss 2.09056, acc 0.4375\n",
      "2016-12-04T11:21:56.774625: step 50, loss 1.80575, acc 0.515625\n",
      "2016-12-04T11:21:57.208938: step 51, loss 1.68958, acc 0.546875\n",
      "2016-12-04T11:21:57.643879: step 52, loss 1.90185, acc 0.421875\n",
      "2016-12-04T11:21:58.078765: step 53, loss 1.82139, acc 0.5625\n",
      "2016-12-04T11:21:58.515125: step 54, loss 1.9071, acc 0.453125\n",
      "2016-12-04T11:21:58.950958: step 55, loss 1.74318, acc 0.5\n",
      "2016-12-04T11:21:59.384424: step 56, loss 1.90905, acc 0.546875\n",
      "2016-12-04T11:21:59.816799: step 57, loss 1.49734, acc 0.515625\n",
      "2016-12-04T11:22:00.255954: step 58, loss 1.44503, acc 0.515625\n",
      "2016-12-04T11:22:00.695101: step 59, loss 1.12819, acc 0.625\n",
      "2016-12-04T11:22:01.134921: step 60, loss 1.22643, acc 0.546875\n",
      "2016-12-04T11:22:01.573506: step 61, loss 1.70871, acc 0.53125\n",
      "2016-12-04T11:22:02.014661: step 62, loss 1.60941, acc 0.5\n",
      "2016-12-04T11:22:02.452033: step 63, loss 1.8527, acc 0.5\n",
      "2016-12-04T11:22:02.890178: step 64, loss 1.74161, acc 0.484375\n",
      "2016-12-04T11:22:03.327282: step 65, loss 1.05763, acc 0.703125\n",
      "2016-12-04T11:22:03.765509: step 66, loss 1.4312, acc 0.5625\n",
      "2016-12-04T11:22:04.200768: step 67, loss 1.52212, acc 0.5\n",
      "2016-12-04T11:22:04.634842: step 68, loss 1.92618, acc 0.359375\n",
      "2016-12-04T11:22:05.069437: step 69, loss 1.33069, acc 0.53125\n",
      "2016-12-04T11:22:05.504947: step 70, loss 1.34087, acc 0.53125\n",
      "2016-12-04T11:22:05.940202: step 71, loss 1.45077, acc 0.515625\n",
      "2016-12-04T11:22:06.377943: step 72, loss 1.36913, acc 0.5625\n",
      "2016-12-04T11:22:06.820103: step 73, loss 1.91128, acc 0.4375\n",
      "2016-12-04T11:22:07.259523: step 74, loss 1.27791, acc 0.5\n",
      "2016-12-04T11:22:07.695455: step 75, loss 1.06445, acc 0.609375\n",
      "2016-12-04T11:22:08.131812: step 76, loss 1.98571, acc 0.515625\n",
      "2016-12-04T11:22:08.569090: step 77, loss 1.15037, acc 0.53125\n",
      "2016-12-04T11:22:09.004706: step 78, loss 1.40044, acc 0.484375\n",
      "2016-12-04T11:22:09.444114: step 79, loss 1.44891, acc 0.546875\n",
      "2016-12-04T11:22:09.882642: step 80, loss 1.46852, acc 0.5\n",
      "2016-12-04T11:22:10.320987: step 81, loss 1.19738, acc 0.5625\n",
      "2016-12-04T11:22:10.756071: step 82, loss 1.61583, acc 0.515625\n",
      "2016-12-04T11:22:11.191354: step 83, loss 1.50643, acc 0.515625\n",
      "2016-12-04T11:22:11.628057: step 84, loss 1.31477, acc 0.546875\n",
      "2016-12-04T11:22:12.063929: step 85, loss 1.40443, acc 0.515625\n",
      "2016-12-04T11:22:12.498139: step 86, loss 1.59197, acc 0.515625\n",
      "2016-12-04T11:22:12.933549: step 87, loss 1.63007, acc 0.484375\n",
      "2016-12-04T11:22:13.371089: step 88, loss 1.21455, acc 0.546875\n",
      "2016-12-04T11:22:13.807932: step 89, loss 1.61394, acc 0.578125\n",
      "2016-12-04T11:22:14.247192: step 90, loss 1.15807, acc 0.546875\n",
      "2016-12-04T11:22:14.684336: step 91, loss 0.840141, acc 0.609375\n",
      "2016-12-04T11:22:15.122326: step 92, loss 1.3868, acc 0.5625\n",
      "2016-12-04T11:22:15.561792: step 93, loss 1.25309, acc 0.53125\n",
      "2016-12-04T11:22:15.998575: step 94, loss 1.25023, acc 0.671875\n",
      "2016-12-04T11:22:16.435847: step 95, loss 1.71324, acc 0.40625\n",
      "2016-12-04T11:22:16.875242: step 96, loss 1.71982, acc 0.515625\n",
      "2016-12-04T11:22:17.310296: step 97, loss 1.30845, acc 0.5\n",
      "2016-12-04T11:22:17.745925: step 98, loss 1.28016, acc 0.625\n",
      "2016-12-04T11:22:18.181669: step 99, loss 1.26286, acc 0.59375\n",
      "2016-12-04T11:22:18.619312: step 100, loss 1.3441, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2016-12-04T11:22:20.208816: step 100, loss 0.927306, acc 0.542214\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/anaconda3/playground_tf/runs/1480850493/checkpoints/model-100\n",
      "\n",
      "2016-12-04T11:22:21.282049: step 101, loss 1.06156, acc 0.609375\n",
      "2016-12-04T11:22:21.722642: step 102, loss 1.54323, acc 0.515625\n",
      "2016-12-04T11:22:22.161468: step 103, loss 1.45301, acc 0.53125\n",
      "2016-12-04T11:22:22.603115: step 104, loss 1.2889, acc 0.5625\n",
      "2016-12-04T11:22:23.042786: step 105, loss 1.43294, acc 0.546875\n",
      "2016-12-04T11:22:23.476988: step 106, loss 1.37216, acc 0.5\n",
      "2016-12-04T11:22:23.913824: step 107, loss 1.64054, acc 0.40625\n",
      "2016-12-04T11:22:24.354785: step 108, loss 1.37842, acc 0.515625\n",
      "2016-12-04T11:22:24.802033: step 109, loss 0.903004, acc 0.65625\n",
      "2016-12-04T11:22:25.239987: step 110, loss 1.14697, acc 0.546875\n",
      "2016-12-04T11:22:25.677085: step 111, loss 1.74975, acc 0.53125\n",
      "2016-12-04T11:22:26.119769: step 112, loss 1.29748, acc 0.515625\n",
      "2016-12-04T11:22:26.559046: step 113, loss 1.46168, acc 0.546875\n",
      "2016-12-04T11:22:26.994880: step 114, loss 1.17714, acc 0.578125\n",
      "2016-12-04T11:22:27.428563: step 115, loss 1.35239, acc 0.609375\n",
      "2016-12-04T11:22:27.866649: step 116, loss 1.55085, acc 0.46875\n",
      "2016-12-04T11:22:28.308046: step 117, loss 1.77734, acc 0.453125\n",
      "2016-12-04T11:22:28.746909: step 118, loss 1.18473, acc 0.546875\n",
      "2016-12-04T11:22:29.188531: step 119, loss 1.14968, acc 0.578125\n",
      "2016-12-04T11:22:29.630159: step 120, loss 1.29632, acc 0.53125\n",
      "2016-12-04T11:22:30.070500: step 121, loss 1.27213, acc 0.53125\n",
      "2016-12-04T11:22:30.512445: step 122, loss 1.66596, acc 0.46875\n",
      "2016-12-04T11:22:30.951216: step 123, loss 1.14852, acc 0.59375\n",
      "2016-12-04T11:22:31.387426: step 124, loss 1.0644, acc 0.59375\n",
      "2016-12-04T11:22:31.824638: step 125, loss 1.10477, acc 0.578125\n",
      "2016-12-04T11:22:32.272270: step 126, loss 1.22109, acc 0.5625\n",
      "2016-12-04T11:22:32.713453: step 127, loss 1.00523, acc 0.578125\n",
      "2016-12-04T11:22:33.151258: step 128, loss 1.13536, acc 0.59375\n",
      "2016-12-04T11:22:33.593120: step 129, loss 1.35726, acc 0.609375\n",
      "2016-12-04T11:22:34.035049: step 130, loss 1.43154, acc 0.578125\n",
      "2016-12-04T11:22:34.479201: step 131, loss 1.51158, acc 0.53125\n",
      "2016-12-04T11:22:34.918352: step 132, loss 1.96562, acc 0.46875\n",
      "2016-12-04T11:22:35.464453: step 133, loss 1.4301, acc 0.59375\n",
      "2016-12-04T11:22:35.962062: step 134, loss 1.32378, acc 0.5\n",
      "2016-12-04T11:22:36.402744: step 135, loss 0.81924, acc 0.578125\n",
      "2016-12-04T11:22:36.907812: step 136, loss 1.83027, acc 0.515625\n",
      "2016-12-04T11:22:37.346359: step 137, loss 1.28575, acc 0.484375\n",
      "2016-12-04T11:22:37.784450: step 138, loss 1.43903, acc 0.5\n",
      "2016-12-04T11:22:38.224260: step 139, loss 1.30388, acc 0.546875\n",
      "2016-12-04T11:22:38.662883: step 140, loss 1.29062, acc 0.546875\n",
      "2016-12-04T11:22:39.101962: step 141, loss 1.1687, acc 0.546875\n",
      "2016-12-04T11:22:39.541426: step 142, loss 1.2625, acc 0.5\n",
      "2016-12-04T11:22:39.980962: step 143, loss 1.00901, acc 0.625\n",
      "2016-12-04T11:22:40.422479: step 144, loss 0.992915, acc 0.578125\n",
      "2016-12-04T11:22:40.863765: step 145, loss 1.02388, acc 0.53125\n",
      "2016-12-04T11:22:41.302907: step 146, loss 1.17357, acc 0.484375\n",
      "2016-12-04T11:22:41.742731: step 147, loss 1.00438, acc 0.65625\n",
      "2016-12-04T11:22:42.187806: step 148, loss 1.47081, acc 0.421875\n",
      "2016-12-04T11:22:42.627118: step 149, loss 1.19077, acc 0.53125\n",
      "2016-12-04T11:22:43.045475: step 150, loss 1.38361, acc 0.566667\n",
      "2016-12-04T11:22:43.485910: step 151, loss 0.912842, acc 0.578125\n",
      "2016-12-04T11:22:43.926202: step 152, loss 0.889219, acc 0.578125\n",
      "2016-12-04T11:22:44.364833: step 153, loss 0.86111, acc 0.640625\n",
      "2016-12-04T11:22:44.804090: step 154, loss 0.984741, acc 0.640625\n",
      "2016-12-04T11:22:45.243638: step 155, loss 0.873603, acc 0.65625\n",
      "2016-12-04T11:22:45.683849: step 156, loss 1.04915, acc 0.578125\n",
      "2016-12-04T11:22:46.123880: step 157, loss 0.972736, acc 0.53125\n",
      "2016-12-04T11:22:46.564496: step 158, loss 0.96485, acc 0.640625\n",
      "2016-12-04T11:22:47.005055: step 159, loss 1.07627, acc 0.515625\n",
      "2016-12-04T11:22:47.443428: step 160, loss 1.06884, acc 0.59375\n",
      "2016-12-04T11:22:47.883793: step 161, loss 0.756322, acc 0.65625\n",
      "2016-12-04T11:22:48.320435: step 162, loss 0.840979, acc 0.625\n",
      "2016-12-04T11:22:48.762682: step 163, loss 1.03337, acc 0.609375\n",
      "2016-12-04T11:22:49.200311: step 164, loss 1.02135, acc 0.578125\n",
      "2016-12-04T11:22:49.637239: step 165, loss 0.611206, acc 0.71875\n",
      "2016-12-04T11:22:50.076816: step 166, loss 0.753493, acc 0.703125\n",
      "2016-12-04T11:22:50.539663: step 167, loss 1.21504, acc 0.546875\n",
      "2016-12-04T11:22:50.976382: step 168, loss 0.740391, acc 0.6875\n",
      "2016-12-04T11:22:51.414795: step 169, loss 1.05613, acc 0.59375\n",
      "2016-12-04T11:22:51.853651: step 170, loss 1.01215, acc 0.640625\n",
      "2016-12-04T11:22:52.295632: step 171, loss 0.854451, acc 0.625\n",
      "2016-12-04T11:22:52.733227: step 172, loss 0.985024, acc 0.5625\n",
      "2016-12-04T11:22:53.170587: step 173, loss 1.06438, acc 0.625\n",
      "2016-12-04T11:22:53.607671: step 174, loss 1.04664, acc 0.609375\n",
      "2016-12-04T11:22:54.046084: step 175, loss 0.743784, acc 0.671875\n",
      "2016-12-04T11:22:54.482347: step 176, loss 0.711631, acc 0.75\n",
      "2016-12-04T11:22:54.920824: step 177, loss 0.661942, acc 0.703125\n",
      "2016-12-04T11:22:55.359847: step 178, loss 0.934253, acc 0.625\n",
      "2016-12-04T11:22:55.797697: step 179, loss 1.11571, acc 0.609375\n",
      "2016-12-04T11:22:56.237461: step 180, loss 1.0157, acc 0.578125\n",
      "2016-12-04T11:22:56.676972: step 181, loss 1.14477, acc 0.46875\n",
      "2016-12-04T11:22:57.116980: step 182, loss 0.722896, acc 0.65625\n",
      "2016-12-04T11:22:57.556533: step 183, loss 0.647648, acc 0.703125\n",
      "2016-12-04T11:22:57.992584: step 184, loss 0.809917, acc 0.6875\n",
      "2016-12-04T11:22:58.432564: step 185, loss 0.693774, acc 0.734375\n",
      "2016-12-04T11:22:58.870173: step 186, loss 0.870973, acc 0.65625\n",
      "2016-12-04T11:22:59.311437: step 187, loss 0.961779, acc 0.578125\n",
      "2016-12-04T11:22:59.750414: step 188, loss 1.01522, acc 0.609375\n",
      "2016-12-04T11:23:00.191657: step 189, loss 0.911207, acc 0.5625\n",
      "2016-12-04T11:23:00.632428: step 190, loss 0.717124, acc 0.71875\n",
      "2016-12-04T11:23:01.072154: step 191, loss 0.849241, acc 0.640625\n",
      "2016-12-04T11:23:01.513844: step 192, loss 1.25946, acc 0.484375\n",
      "2016-12-04T11:23:01.954709: step 193, loss 0.971925, acc 0.609375\n",
      "2016-12-04T11:23:02.397555: step 194, loss 0.934284, acc 0.625\n",
      "2016-12-04T11:23:02.841349: step 195, loss 0.832904, acc 0.625\n",
      "2016-12-04T11:23:03.287713: step 196, loss 0.817326, acc 0.625\n",
      "2016-12-04T11:23:03.730901: step 197, loss 0.945932, acc 0.53125\n",
      "2016-12-04T11:23:04.168596: step 198, loss 1.24725, acc 0.453125\n",
      "2016-12-04T11:23:04.606542: step 199, loss 0.881949, acc 0.609375\n",
      "2016-12-04T11:23:05.043292: step 200, loss 0.703322, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2016-12-04T11:23:06.655778: step 200, loss 0.67593, acc 0.617261\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/anaconda3/playground_tf/runs/1480850493/checkpoints/model-200\n",
      "\n",
      "2016-12-04T11:23:07.712978: step 201, loss 0.805403, acc 0.671875\n",
      "2016-12-04T11:23:08.152424: step 202, loss 0.637677, acc 0.703125\n",
      "2016-12-04T11:23:08.592470: step 203, loss 0.929846, acc 0.671875\n",
      "2016-12-04T11:23:09.034093: step 204, loss 0.889036, acc 0.671875\n",
      "2016-12-04T11:23:09.476224: step 205, loss 1.07477, acc 0.59375\n",
      "2016-12-04T11:23:09.917182: step 206, loss 0.865009, acc 0.5625\n",
      "2016-12-04T11:23:10.353862: step 207, loss 0.733216, acc 0.703125\n",
      "2016-12-04T11:23:10.797177: step 208, loss 0.662157, acc 0.734375\n",
      "2016-12-04T11:23:11.236783: step 209, loss 0.837653, acc 0.640625\n",
      "2016-12-04T11:23:11.675343: step 210, loss 0.973684, acc 0.5625\n",
      "2016-12-04T11:23:12.109739: step 211, loss 0.889832, acc 0.59375\n",
      "2016-12-04T11:23:12.547784: step 212, loss 1.08887, acc 0.578125\n",
      "2016-12-04T11:23:12.988342: step 213, loss 0.985383, acc 0.578125\n",
      "2016-12-04T11:23:13.427786: step 214, loss 0.869986, acc 0.625\n",
      "2016-12-04T11:23:13.860400: step 215, loss 0.889774, acc 0.46875\n",
      "2016-12-04T11:23:14.298694: step 216, loss 0.86613, acc 0.609375\n",
      "2016-12-04T11:23:14.734630: step 217, loss 0.933122, acc 0.53125\n",
      "2016-12-04T11:23:15.171307: step 218, loss 0.955546, acc 0.625\n",
      "2016-12-04T11:23:15.606462: step 219, loss 1.06442, acc 0.625\n",
      "2016-12-04T11:23:16.043034: step 220, loss 1.08332, acc 0.53125\n",
      "2016-12-04T11:23:16.483576: step 221, loss 0.917133, acc 0.53125\n",
      "2016-12-04T11:23:16.922946: step 222, loss 0.726789, acc 0.71875\n",
      "2016-12-04T11:23:17.359936: step 223, loss 0.880754, acc 0.703125\n",
      "2016-12-04T11:23:17.798112: step 224, loss 0.749035, acc 0.671875\n",
      "2016-12-04T11:23:18.236121: step 225, loss 0.687847, acc 0.65625\n",
      "2016-12-04T11:23:18.675177: step 226, loss 0.707899, acc 0.703125\n",
      "2016-12-04T11:23:19.115113: step 227, loss 0.95172, acc 0.578125\n",
      "2016-12-04T11:23:19.553888: step 228, loss 0.855232, acc 0.578125\n",
      "2016-12-04T11:23:19.991402: step 229, loss 1.20846, acc 0.515625\n",
      "2016-12-04T11:23:20.430745: step 230, loss 1.15343, acc 0.5625\n",
      "2016-12-04T11:23:20.869848: step 231, loss 0.690215, acc 0.625\n",
      "2016-12-04T11:23:21.307975: step 232, loss 0.860853, acc 0.625\n",
      "2016-12-04T11:23:21.746148: step 233, loss 0.67017, acc 0.65625\n",
      "2016-12-04T11:23:22.182504: step 234, loss 0.68649, acc 0.65625\n",
      "2016-12-04T11:23:22.623853: step 235, loss 0.805705, acc 0.609375\n",
      "2016-12-04T11:23:23.065017: step 236, loss 0.920396, acc 0.53125\n",
      "2016-12-04T11:23:23.504731: step 237, loss 1.1418, acc 0.53125\n",
      "2016-12-04T11:23:23.941483: step 238, loss 0.898782, acc 0.578125\n",
      "2016-12-04T11:23:24.380190: step 239, loss 0.90768, acc 0.53125\n",
      "2016-12-04T11:23:24.823558: step 240, loss 1.14315, acc 0.546875\n",
      "2016-12-04T11:23:25.264480: step 241, loss 0.564414, acc 0.734375\n",
      "2016-12-04T11:23:25.704247: step 242, loss 0.777382, acc 0.65625\n",
      "2016-12-04T11:23:26.141098: step 243, loss 0.74004, acc 0.640625\n",
      "2016-12-04T11:23:26.578492: step 244, loss 0.70863, acc 0.671875\n",
      "2016-12-04T11:23:27.012880: step 245, loss 0.916454, acc 0.59375\n",
      "2016-12-04T11:23:27.447951: step 246, loss 0.899416, acc 0.515625\n",
      "2016-12-04T11:23:27.883479: step 247, loss 0.965053, acc 0.5625\n",
      "2016-12-04T11:23:28.322358: step 248, loss 0.874645, acc 0.59375\n",
      "2016-12-04T11:23:28.764491: step 249, loss 0.921893, acc 0.671875\n",
      "2016-12-04T11:23:29.198801: step 250, loss 0.752658, acc 0.640625\n",
      "2016-12-04T11:23:29.634583: step 251, loss 0.874451, acc 0.609375\n",
      "2016-12-04T11:23:30.068348: step 252, loss 1.02456, acc 0.59375\n",
      "2016-12-04T11:23:30.502545: step 253, loss 0.845017, acc 0.59375\n",
      "2016-12-04T11:23:30.936237: step 254, loss 0.792229, acc 0.625\n",
      "2016-12-04T11:23:31.370656: step 255, loss 0.708853, acc 0.640625\n",
      "2016-12-04T11:23:31.806767: step 256, loss 0.767397, acc 0.59375\n",
      "2016-12-04T11:23:32.244075: step 257, loss 0.901708, acc 0.59375\n",
      "2016-12-04T11:23:32.681853: step 258, loss 0.87643, acc 0.5625\n",
      "2016-12-04T11:23:33.117213: step 259, loss 0.715531, acc 0.6875\n",
      "2016-12-04T11:23:33.562204: step 260, loss 0.881235, acc 0.59375\n",
      "2016-12-04T11:23:34.002098: step 261, loss 0.834454, acc 0.578125\n",
      "2016-12-04T11:23:34.440253: step 262, loss 0.827395, acc 0.515625\n",
      "2016-12-04T11:23:34.878667: step 263, loss 0.967049, acc 0.5625\n",
      "2016-12-04T11:23:35.315046: step 264, loss 0.72025, acc 0.640625\n",
      "2016-12-04T11:23:35.750719: step 265, loss 0.717346, acc 0.6875\n",
      "2016-12-04T11:23:36.188336: step 266, loss 0.583738, acc 0.6875\n",
      "2016-12-04T11:23:36.626116: step 267, loss 1.02012, acc 0.53125\n",
      "2016-12-04T11:23:37.064143: step 268, loss 0.717316, acc 0.640625\n",
      "2016-12-04T11:23:37.500959: step 269, loss 0.691417, acc 0.609375\n",
      "2016-12-04T11:23:37.940993: step 270, loss 0.645782, acc 0.703125\n",
      "2016-12-04T11:23:38.374845: step 271, loss 0.721635, acc 0.625\n",
      "2016-12-04T11:23:38.811502: step 272, loss 0.926507, acc 0.578125\n",
      "2016-12-04T11:23:39.248429: step 273, loss 1.0847, acc 0.546875\n",
      "2016-12-04T11:23:39.684065: step 274, loss 0.686933, acc 0.6875\n",
      "2016-12-04T11:23:40.121537: step 275, loss 0.837544, acc 0.625\n",
      "2016-12-04T11:23:40.554473: step 276, loss 0.666153, acc 0.625\n",
      "2016-12-04T11:23:40.990188: step 277, loss 0.833983, acc 0.625\n",
      "2016-12-04T11:23:41.424972: step 278, loss 0.777628, acc 0.625\n",
      "2016-12-04T11:23:41.862148: step 279, loss 0.814592, acc 0.671875\n",
      "2016-12-04T11:23:42.299190: step 280, loss 0.556698, acc 0.71875\n",
      "2016-12-04T11:23:42.734597: step 281, loss 0.810167, acc 0.640625\n",
      "2016-12-04T11:23:43.171902: step 282, loss 0.789544, acc 0.609375\n",
      "2016-12-04T11:23:43.607577: step 283, loss 0.879763, acc 0.5\n",
      "2016-12-04T11:23:44.048709: step 284, loss 0.488797, acc 0.796875\n",
      "2016-12-04T11:23:44.487813: step 285, loss 0.696368, acc 0.609375\n",
      "2016-12-04T11:23:44.928155: step 286, loss 0.817489, acc 0.546875\n",
      "2016-12-04T11:23:45.366684: step 287, loss 0.751675, acc 0.640625\n",
      "2016-12-04T11:23:45.806286: step 288, loss 1.01898, acc 0.546875\n",
      "2016-12-04T11:23:46.245840: step 289, loss 0.767594, acc 0.546875\n",
      "2016-12-04T11:23:46.683772: step 290, loss 0.870189, acc 0.578125\n",
      "2016-12-04T11:23:47.122819: step 291, loss 0.629284, acc 0.6875\n",
      "2016-12-04T11:23:47.560124: step 292, loss 0.838708, acc 0.515625\n",
      "2016-12-04T11:23:47.997407: step 293, loss 0.73174, acc 0.65625\n",
      "2016-12-04T11:23:48.438925: step 294, loss 0.719526, acc 0.6875\n",
      "2016-12-04T11:23:48.875678: step 295, loss 0.80454, acc 0.640625\n",
      "2016-12-04T11:23:49.316484: step 296, loss 0.792688, acc 0.640625\n",
      "2016-12-04T11:23:49.754345: step 297, loss 0.760262, acc 0.625\n",
      "2016-12-04T11:23:50.192459: step 298, loss 0.84148, acc 0.59375\n",
      "2016-12-04T11:23:50.633449: step 299, loss 0.767116, acc 0.609375\n",
      "2016-12-04T11:23:51.049124: step 300, loss 0.739767, acc 0.6\n",
      "\n",
      "Evaluation:\n",
      "2016-12-04T11:23:52.597324: step 300, loss 0.649114, acc 0.629456\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/anaconda3/playground_tf/runs/1480850493/checkpoints/model-300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
