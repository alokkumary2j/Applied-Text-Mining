{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "DEFINE_integer(flag_name, default_value, docstring):\n",
    " Defines a flag of type 'int'.Args:\n",
    "   flag_name: The name of the flag as a string.\n",
    "   default_value: The default value the flag should take as an int.\n",
    "   docstring: A helpful message explaining the use of the flag.\n",
    "\"\"\"\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the positive data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 2, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_EPOCHS=2\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "Loading data...Started\n",
      "Loading data...Finished\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "# Data Preparation\n",
    "# ===========================================================================\n",
    "# Load data\n",
    "print(\"Loading data...Started\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "print(\"Loading data...Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10662,\n",
       " array([ 1,  2,  3,  4,  5,  6,  1,  7,  8,  9, 10, 11, 12, 13, 14,  9, 15,\n",
       "         5, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])#56 for Current Dataset\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#vocab_processor.min_frequency,vocab_processor.max_document_length ~~> 0,56\n",
    "#vocab_processor.vocabulary_.__dict__ ~~>{'_freeze': False, '_freq': defaultdict(int, {}), '_mapping': {'<UNK>': 0},\n",
    "# '_reverse_mapping': ['<UNK>'], '_support_reverse': True,  '_unknown_token': '<UNK>'}\n",
    "trainedVocab_Processor=vocab_processor.fit_transform(x_text)\n",
    "listTrainedVocab=list(trainedVocab_Processor)\n",
    "\"\"\"\n",
    "len(listTrainedVocab)~>10662\n",
    "x_text[0] ~> \"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even \n",
    "greater than arnold schwarzenegger , jean claud van damme or steven segal\"\n",
    "listTrainedVocab[0] ~> array([ 1,  2,  3,  4,  5,  6,  1,  7,  8,  9, 10, 11, 12, 13, 14,  9, 15,5, 16, 17, 18, 19, 20, \n",
    "21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 0,  0,  \n",
    "0,  0,  0]) \n",
    "listTrainedVocab[1] ~> array([ 1, 31, 32, 33, 34,  1, 35, 34,  1, 36, 37,  3, 38, 39, 13, 17, 40, 34, 41, 42, 43, 44, 45,\n",
    "46, 47, 48, 49,  9, 50, 51, 34, 52, 53, 53, 54,  9, 55, 56,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 0,  0,  \n",
    "0,  0,  0])\n",
    "x_text[1] ~> \"the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words \n",
    "cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\"\n",
    "len(listTrainedVocab[0]) ~> 56\n",
    "len(listTrainedVocab[1] ~> 56\n",
    "x_text[10661]~> \"enigma is well made , but it 's just too dry and too placid\"\n",
    "listTrainedVocab[10661] ~> array([11512,     3,   147,   113,    58,    84,     9,   655,    59,2766,    12,    59,  8453,     0,     0,     0,     \n",
    " 0,0, 0,     0,     0,     0,     0,     0,     0,     0,     0,0,     0,     0,     0,     0,     0,     0,     0,     \n",
    " 0, 0,     0,     0,     0,     0,     0,     0,     0,     0, 0,0, 0,0,0,0,     0,     0,     0, 0,     0])\n",
    "listTrainedVocab[10659]~> array([   75,    84,  1949, 10191,  2045,   114,     1, 18755, 12889,1293,835,34, 1, 18756,  \n",
    "5333,188,  1682,  1334,34,    17,  4317,  2490,   996,   121, 12311,  8524,    34,7369, 12085, 18757,   419,     1,  \n",
    "2490,     9,  1473,    34,  7327, 0,0,0,0, 0,0,0, 0, 0,0, 0, 0,0,0, 0,0,0,0,0])\n",
    "\"\"\"\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "len(x),x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1066 10662\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))#Picking 10% of indexes from end\n",
    "print(dev_sample_index,len(y))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_unknown_token', '_reverse_mapping', '_freeze', '_mapping', '_support_reverse', '_freq'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictVocab=vocab_processor.vocabulary_\n",
    "type(dictVocab.__dict__)\n",
    "dictVocab.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9596, 56) (9596, 2) 18758 128\n",
      "[3, 4, 5] 128 0.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,len(vocab_processor.vocabulary_),FLAGS.embedding_dim)\n",
    "print(list(map(int, FLAGS.filter_sizes.split(\",\"))),FLAGS.num_filters,FLAGS.l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length=x_train.shape[1]\n",
    "num_classes=y_train.shape[1]\n",
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "embedding_size=FLAGS.embedding_dim\n",
    "filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\")))\n",
    "num_filters=FLAGS.num_filters\n",
    "l2_reg_lambda=FLAGS.l2_reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.placeholder creates a placeholder variable and feeded to the network when we execute it at train/test time.\n",
    "#Second argument~>shape of tensor(None means that the length of that dimension could be anything)\n",
    "#Using None allows the network to handle arbitrarily sized batches.\n",
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "#The probability of keeping a neuron in the dropout layer is also an input to the network because \n",
    "#we enable dropout only during training. We disable it while evaluating the model.\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18758 128\n",
      "W :  Tensor(\"embedding/W/read:0\", shape=(18758, 128), dtype=float32, device=/device:CPU:0)\n",
      "Input_x[0] :  Tensor(\"embedding/strided_slice:0\", shape=(56,), dtype=int32, device=/device:CPU:0)\n",
      "Embedded Chars :  Tensor(\"embedding/embedding_lookup:0\", shape=(?, 56, 128), dtype=float32, device=/device:CPU:0)\n",
      "Expanded Embedded Chars :  Tensor(\"embedding/ExpandDims:0\", shape=(?, 56, 128, 1), dtype=float32, device=/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "\"\"\"\n",
    "The first layer we define is the embedding layer, which maps vocabulary word indices into low-dimensional \n",
    "vector representations. It’s essentially a lookup table that we learn from data.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "tf.device(\"/cpu:0\") forces an operation to be executed on the CPU. By default TensorFlow will try to put the \n",
    "operation on the GPU if one is available, but the embedding implementation doesn’t currently have GPU support \n",
    "and throws an error if placed on the GPU.\n",
    "tf.name_scope creates a new Name Scope with the name “embedding”. The scope adds all operations into a top-level \n",
    "node called “embedding” so that we get a nice hierarchy when visualizing our network in TensorBoard.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "W is our embedding matrix that we learn during training. We initialize it using a random uniform distribution. \n",
    "\n",
    "tf.nn.embedding_lookup creates the actual embedding operation. The result of the embedding operation is a \n",
    "3-dimensional tensor of shape [None, sequence_length, embedding_size].\n",
    "\n",
    "TensorFlow’s convolutional conv2d operation expects a 4-dimensional tensor with dimensions corresponding to \n",
    "batch, width, height and channel. The result of our embedding doesn’t contain the channel dimension, so we \n",
    "add it manually, leaving us with a layer of shape [None, sequence_length, embedding_size, 1].\n",
    "\"\"\"\n",
    "print(vocab_size,embedding_size)#18758,128\n",
    "embedded_chars=None\n",
    "embedded_chars_expanded=None\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "        name=\"W\")#W has shape [18758,128]~>[vocab_size,embedding_size]\n",
    "    print(\"W : \",W.value())\n",
    "    print(\"Input_x[0] : \",input_x[0])\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x)#[18758,128],[None,56]\n",
    "    print(\"Embedded Chars : \",embedded_chars)#[None,56,128]\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(\"Expanded Embedded Chars : \",embedded_chars_expanded)#[None,56,128,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Shape (filter_size, embedding_size, 1, num_filters) :  [3, 128, 1, 128]\n",
      "W :  (3, 128, 1, 128)\n",
      "B :  (128,)\n",
      "Embedded Chars :  (?, 56, 128, 1)\n",
      "Conv :  (?, 54, 1, 128)\n",
      "b :  (128,)\n",
      "conv+b :  (?, 54, 1, 128)\n",
      "h :  (?, 54, 1, 128)\n",
      "sequence_length, filter_size (56, 3)\n",
      "ksize[1, sequence_length - filter_size + 1, 1, 1]~> [1, 54, 1, 1]\n",
      "pooled :  (?, 1, 1, 128)\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Filter Shape (filter_size, embedding_size, 1, num_filters) :  [4, 128, 1, 128]\n",
      "W :  (4, 128, 1, 128)\n",
      "B :  (128,)\n",
      "Embedded Chars :  (?, 56, 128, 1)\n",
      "Conv :  (?, 53, 1, 128)\n",
      "b :  (128,)\n",
      "conv+b :  (?, 53, 1, 128)\n",
      "h :  (?, 53, 1, 128)\n",
      "sequence_length, filter_size (56, 4)\n",
      "ksize[1, sequence_length - filter_size + 1, 1, 1]~> [1, 53, 1, 1]\n",
      "pooled :  (?, 1, 1, 128)\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Filter Shape (filter_size, embedding_size, 1, num_filters) :  [5, 128, 1, 128]\n",
      "W :  (5, 128, 1, 128)\n",
      "B :  (128,)\n",
      "Embedded Chars :  (?, 56, 128, 1)\n",
      "Conv :  (?, 52, 1, 128)\n",
      "b :  (128,)\n",
      "conv+b :  (?, 52, 1, 128)\n",
      "h :  (?, 52, 1, 128)\n",
      "sequence_length, filter_size (56, 5)\n",
      "ksize[1, sequence_length - filter_size + 1, 1, 1]~> [1, 52, 1, 1]\n",
      "pooled :  (?, 1, 1, 128)\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "# Create a convolution + maxpool layer for each filter size\n",
    "\"\"\"\n",
    "We use filters of different sizes. Because each convolution produces tensors of different shapes we need to \n",
    "iterate through them, create a layer for each of them, and then merge the results into one big feature vector.\n",
    "\n",
    "W :filter matrix and h is the result of applying the nonlinearity to the convolution output. \n",
    "Each filter slides over the whole embedding, but varies in how many words it covers. \n",
    "\n",
    "\"VALID\" padding means that we slide the filter over our sentence without padding the edges, performing a narrow \n",
    "convolution that gives us an output of shape [1, sequence_length - filter_size + 1, 1, 1]. \n",
    "\n",
    "Performing max-pooling over the output of a specific filter size leaves us with a tensor of shape \n",
    "[batch_size, 1, 1, num_filters]. This is essentially a feature vector, where the last dimension corresponds to our \n",
    "features. Once we have all the pooled output tensors from each filter size we combine them into one long feature \n",
    "vector of shape [batch_size, num_filters_total]. \n",
    "\n",
    "Using -1 in tf.reshape tells TensorFlow to flatten the dimension when possible.\n",
    "\"\"\"\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        # Convolution Layer\n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        print(\"Filter Shape (filter_size, embedding_size, 1, num_filters) : \",filter_shape)\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        print(\"W : \",W.get_shape())\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "        print(\"B : \",b.get_shape())\n",
    "        print(\"Embedded Chars : \",embedded_chars_expanded.get_shape())\n",
    "        conv = tf.nn.conv2d(\n",
    "            embedded_chars_expanded,\n",
    "            W,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"conv\")\n",
    "        print(\"Conv : \",conv.get_shape())\n",
    "        print(\"b : \",b.get_shape())\n",
    "        # Apply nonlinearity\n",
    "        conv_Plus_b=tf.nn.bias_add(conv, b)\n",
    "        print(\"conv+b : \",conv_Plus_b.get_shape())\n",
    "        h = tf.nn.relu(conv_Plus_b, name=\"relu\")\n",
    "        print(\"h : \",h.get_shape())\n",
    "        # Maxpooling over the outputs\n",
    "        print(\"sequence_length, filter_size\",(sequence_length , filter_size))\n",
    "        print(\"ksize[1, sequence_length - filter_size + 1, 1, 1]~>\",[1, sequence_length - filter_size + 1, 1, 1])\n",
    "        pooled = tf.nn.max_pool(\n",
    "            h,\n",
    "            ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='VALID',\n",
    "            name=\"pool\")\n",
    "        print(\"pooled : \",pooled.get_shape())\n",
    "        pooled_outputs.append(pooled)\n",
    "        print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_filters,numOfFilters,num_filters_total ~>  (128, 3, 384)\n",
      "pooled_outputs :  3 (?, 1, 1, 128) (?, 1, 1, 128) (?, 1, 1, 128)\n",
      "h_pool ~>  (?, 1, 1, 384)\n",
      "h_pool_flat ~>  (?, 384)\n"
     ]
    }
   ],
   "source": [
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "print(\"num_filters,numOfFilters,num_filters_total ~> \",(num_filters,len(filter_sizes),num_filters_total))\n",
    "print(\"pooled_outputs : \",len(pooled_outputs),pooled_outputs[0].get_shape(),pooled_outputs[1].get_shape(),\n",
    "      pooled_outputs[2].get_shape())\n",
    "h_pool = tf.concat(3, pooled_outputs)\n",
    "print(\"h_pool ~> \",h_pool.get_shape())\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "print(\"h_pool_flat ~> \",h_pool_flat.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_keep_prob :  Tensor(\"dropout_keep_prob:0\", dtype=float32)\n",
      "h_pool_flat :  (?, 384)\n",
      "h_drop :  (?, 384)\n"
     ]
    }
   ],
   "source": [
    "# Add dropout\n",
    "h_drop=None\n",
    "print(\"dropout_keep_prob : \",dropout_keep_prob)\n",
    "print(\"h_pool_flat : \",h_pool_flat.get_shape())\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat,dropout_keep_prob)\n",
    "print(\"h_drop : \",h_drop.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_variable in module tensorflow.python.ops.variable_scope:\n",
      "\n",
      "get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)\n",
      "    Gets an existing variable with these parameters or create a new one.\n",
      "    \n",
      "    This function prefixes the name with the current variable scope\n",
      "    and performs reuse checks. See the\n",
      "    [Variable Scope How To](../../how_tos/variable_scope/index.md)\n",
      "    for an extensive description of how reusing works. Here is a basic example:\n",
      "    \n",
      "    ```python\n",
      "    with tf.variable_scope(\"foo\"):\n",
      "        v = tf.get_variable(\"v\", [1])  # v.name == \"foo/v:0\"\n",
      "        w = tf.get_variable(\"w\", [1])  # w.name == \"foo/w:0\"\n",
      "    with tf.variable_scope(\"foo\", reuse=True)\n",
      "        v1 = tf.get_variable(\"v\")  # The same as v above.\n",
      "    ```\n",
      "    \n",
      "    If initializer is `None` (the default), the default initializer passed in\n",
      "    the variable scope will be used. If that one is `None` too, a\n",
      "    `uniform_unit_scaling_initializer` will be used. The initializer can also be\n",
      "    a Tensor, in which case the variable is initialized to this value and shape.\n",
      "    \n",
      "    Similarly, if the regularizer is `None` (the default), the default regularizer\n",
      "    passed in the variable scope will be used (if that is `None` too,\n",
      "    then by default no regularization is performed).\n",
      "    \n",
      "    If a partitioner is provided, first a sharded `Variable` is created\n",
      "    via `_get_partitioned_variable`, and the return value is a\n",
      "    `Tensor` composed of the shards concatenated along the partition axis.\n",
      "    \n",
      "    Some useful partitioners are available.  See, e.g.,\n",
      "    `variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n",
      "    \n",
      "    Args:\n",
      "      name: The name of the new or existing variable.\n",
      "      shape: Shape of the new or existing variable.\n",
      "      dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).\n",
      "      initializer: Initializer for the variable if one is created.\n",
      "      regularizer: A (Tensor -> Tensor or None) function; the result of\n",
      "        applying it on a newly created variable will be added to the collection\n",
      "        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n",
      "      trainable: If `True` also add the variable to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
      "      collections: List of graph collections keys to add the Variable to.\n",
      "        Defaults to `[GraphKeys.VARIABLES]` (see tf.Variable).\n",
      "      caching_device: Optional device string or function describing where the\n",
      "        Variable should be cached for reading.  Defaults to the Variable's\n",
      "        device.  If not `None`, caches on another device.  Typical use is to\n",
      "        cache on the device where the Ops using the Variable reside, to\n",
      "        deduplicate copying through `Switch` and other conditional statements.\n",
      "      partitioner: Optional callable that accepts a fully defined `TensorShape`\n",
      "        and `dtype` of the Variable to be created, and returns a list of\n",
      "        partitions for each axis (currently only one axis can be partitioned).\n",
      "      validate_shape: If False, allows the variable to be initialized with a\n",
      "          value of unknown shape. If True, the default, the shape of initial_value\n",
      "          must be known.\n",
      "      custom_getter: Callable that takes as a first argument the true getter, and\n",
      "        allows overwriting the internal get_variable method.\n",
      "        The signature of `custom_getter` should match that of this method,\n",
      "        but the most future-proof version will allow for changes:\n",
      "        `def custom_getter(getter, *args, **kwargs)`.  Direct access to\n",
      "        all `get_variable` parameters is also allowed:\n",
      "        `def custom_getter(getter, name, *args, **kwargs)`.  A simple identity\n",
      "        custom getter that simply creates variables with modified names is:\n",
      "        ```python\n",
      "        def custom_getter(getter, name, *args, **kwargs):\n",
      "          return getter(name + '_suffix', *args, **kwargs)\n",
      "        ```\n",
      "    \n",
      "    Returns:\n",
      "      The created or existing variable.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: when creating a new variable and shape is not declared,\n",
      "        when violating reuse during variable creation, or when `initializer` dtype\n",
      "        and `dtype` don't match. Reuse is set inside `variable_scope`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.get_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[num_filters_total, num_classes] :  [384, 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-42-cb99040426a7>\", line 11, in <module>\n    initializer=tf_initializer)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8e6ea85b1578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_filters_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0ml2_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m   1020\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    847\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    343\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    328\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    631\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 633\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-42-cb99040426a7>\", line 11, in <module>\n    initializer=tf_initializer)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# Final (unnormalized) scores and predictions\n",
    "scores=None\n",
    "predictions=None\n",
    "with tf.name_scope(\"output\"):\n",
    "    #tf_initializer=tf.contrib.layers.xavier_initializer()\n",
    "    #print(\"tf_initializer : \",tf_initializer)\n",
    "    print(\"[num_filters_total, num_classes] : \",[num_filters_total, num_classes])\n",
    "    W_1 = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[num_filters_total, num_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6d11c5a61558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Second argument~>shape of tensor(None means that the length of that dimension could be anything)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Using None allows the network to handle arbitrarily sized batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#The probability of keeping a neuron in the dropout layer is also an input to the network because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_length' is not defined"
     ]
    }
   ],
   "source": [
    "# tf.placeholder creates a placeholder variable and feeded to the network when we execute it at train/test time.\n",
    "#Second argument~>shape of tensor(None means that the length of that dimension could be anything)\n",
    "#Using None allows the network to handle arbitrarily sized batches.\n",
    "sequence_length=x_train.shape[1]\n",
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "#The probability of keeping a neuron in the dropout layer is also an input to the network because \n",
    "#we enable dropout only during training. We disable it while evaluating the model.\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        sequence_length – Length of our sentences. All sentences are padded to have the same length[Say 59].\n",
    "        num_classes - Number of classes in the output layer, two in our case (positive and negative).\n",
    "        embedding_size – Dimensionality of the Embedding layer\n",
    "        vocab_size – Size of vocabulary. Defines the size of Embedding layer with shape [vocab_size, embedding_size].\n",
    "        num_filters – Number of filters per filter size\n",
    "        filter_sizes - number of words we want Convolutional filters to cover. We will have num_filters for each size \n",
    "        specified here. For example, [3, 4, 5] means that we will have filters that slide over 3, 4 and 5 words \n",
    "        respectively, for a total of len(filter_sizes) * num_filters filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,embedding_size, filter_sizes, \n",
    "                 num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout [Pay Special attention to the \"self.\"]\n",
    "        # tf.placeholder creates a placeholder variable and feeded to the network when we execute it at train/test time.\n",
    "        #Second argument~>shape of tensor(None means that the length of that dimension could be anything)\n",
    "        #Using None allows the network to handle arbitrarily sized batches.\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        #The probability of keeping a neuron in the dropout layer is also an input to the network because \n",
    "        #we enable dropout only during training. We disable it while evaluating the model.\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        \"\"\"\n",
    "        The first layer we define is the embedding layer, which maps vocabulary word indices into low-dimensional \n",
    "        vector representations. It’s essentially a lookup table that we learn from data.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        tf.device(\"/cpu:0\") forces an operation to be executed on the CPU. By default TensorFlow will try to put the \n",
    "        operation on the GPU if one is available, but the embedding implementation doesn’t currently have GPU support \n",
    "        and throws an error if placed on the GPU.\n",
    "        tf.name_scope creates a new Name Scope with the name “embedding”. The scope adds all operations into a top-level \n",
    "        node called “embedding” so that we get a nice hierarchy when visualizing our network in TensorBoard.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        W is our embedding matrix that we learn during training. We initialize it using a random uniform distribution. \n",
    "        \n",
    "        tf.nn.embedding_lookup creates the actual embedding operation. The result of the embedding operation is a \n",
    "        3-dimensional tensor of shape [None, sequence_length, embedding_size].\n",
    "        \n",
    "        TensorFlow’s convolutional conv2d operation expects a 4-dimensional tensor with dimensions corresponding to \n",
    "        batch, width, height and channel. The result of our embedding doesn’t contain the channel dimension, so we \n",
    "        add it manually, leaving us with a layer of shape [None, sequence_length, embedding_size, 1].\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        \"\"\"\n",
    "        We use filters of different sizes. Because each convolution produces tensors of different shapes we need to \n",
    "        iterate through them, create a layer for each of them, and then merge the results into one big feature vector.\n",
    "        \n",
    "        W :filter matrix and h is the result of applying the nonlinearity to the convolution output. \n",
    "        Each filter slides over the whole embedding, but varies in how many words it covers. \n",
    "        \n",
    "        \"VALID\" padding means that we slide the filter over our sentence without padding the edges, performing a narrow \n",
    "        convolution that gives us an output of shape [1, sequence_length - filter_size + 1, 1, 1]. \n",
    "        \n",
    "        Performing max-pooling over the output of a specific filter size leaves us with a tensor of shape \n",
    "        [batch_size, 1, 1, num_filters]. This is essentially a feature vector, where the last dimension corresponds to our \n",
    "        features. Once we have all the pooled output tensors from each filter size we combine them into one long feature \n",
    "        vector of shape [batch_size, num_filters_total]. \n",
    "        \n",
    "        Using -1 in tf.reshape tells TensorFlow to flatten the dimension when possible.\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
